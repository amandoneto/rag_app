from langchain_core.documents.base import Document
from collections.abc import Iterator
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from utils.env_loader import EnvLoader
from langchain_core.retrievers import BaseRetriever

class RAGApplication:
    """
    A RAG (Retrieval Augmented Generation) application for chatting with local PDF documents.
    Run Pipeline (Search + LLM)
    Costs tokens/time
    """

    def __init__(self, file_path: str):
        """
        Initialize the RAG application.

        Args:
            file_path (str): The path to the PDF file to be processed.
        """
        self.file_path = file_path
        self.vector_store = None
        self.qa_chain = None
        self.config = EnvLoader()
        self._setup()

    def split_document(self, docs: list[Document]) -> list[Document]:
        """
        Splits the document into chunks.

        Returns:
            list[Document]: List of split documents.
        """
        # Using RecursiveCharacterTextSplitter is a best practice for text data
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ".", " ", ""]
        )
        splits = splitter.split_documents(docs)
        return splits
    
    def load_document(self) -> list[Document]:
        """
        Loads the document.

        Returns:
            list[Document]: List of loaded documents.
        """
        # 1. Load the PDF
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"The file {self.file_path} was not found.")

        loader = PyPDFLoader(self.file_path)
        docs = loader.load()
        return docs

    def create_vector_store(self, docs: list[Document]) -> FAISS:
        """
        Creates a vector store from the document and save it in memory.

        Returns:
            FAISS: The vector store.
        """
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.from_documents(docs, embeddings)
        return vector_store

    def create_retriever(self) -> BaseRetriever:
        """
        Creates a retriever from the vector store.

        Returns:
            BaseRetriever: The retriever object.
        """
        retriever = self.vector_store.as_retriever()
        return retriever

    def create_llm(self) -> ChatOpenAI:
        """
        Creates a language model.

        Returns:
            ChatOpenAI: The language model instance.
        """
        llm = ChatOpenAI(model=self.config.get_required("OPENAI_MODEL_NAME"), 
            temperature=0, 
            openai_api_key=self.config.get_required("OPENAI_API_KEY"))
        return llm
    
    def create_qa_chain(self, retriever: BaseRetriever):
        """
        Creates a QA chain from the retriever.
        """
        llm = self.create_llm()
        
        prompt = ChatPromptTemplate.from_template("""
        Answer the following question based only on the provided context:

        <context>
        {context}
        </context>

        Question: {input}
        """)

        document_chain = create_stuff_documents_chain(llm, prompt)        
        self.qa_chain = create_retrieval_chain(retriever, document_chain)

    def _setup(self):
        """
        Sets up the RAG pipeline: loads data, creates embeddings, builds vector store,
        and initializes the retrieval chain.
        """
        
        docs = self.load_document()

        # 2. Split the document
        splits = self.split_document(docs)

        # 3. Create Vector Store
        self.vector_store = self.create_vector_store(splits)
        
        # 4. Create Retriever
        retriever = self.create_retriever()
        
        self.create_qa_chain(retriever)

    def ask(self, question: str) -> str:
        """
        Ask a question to the RAG application.

        Args:
            question (str): The user's question.

        Returns:
            str: The answer generated by the model.
        """
        if not self.qa_chain:
            return "The application is not fully initialized."
        
        response = self.qa_chain.invoke({"input": question})
        return response["answer"]

    def ask_stream(self, question: str) -> Iterator[str]:
        """
        Ask a question to the RAG application and return a stream.

        Args:
            question (str): The user's question.

        Args:
            question (str): The user's question.

        Yields:
            Iterator[str]: Chunks of the answer generated by the model.
        """
        if not self.qa_chain:
            yield "The application is not fully initialized."
            return

        for chunk in self.qa_chain.stream({"input": question}):
            if "answer" in chunk:
                yield chunk["answer"]
